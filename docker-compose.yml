version: '3.8'

services:
  # Kafka Cluster
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - hive

  kafka:
    container_name: kafka
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_HOST://kafka:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_HOST
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
      - "29092:29092"
    networks:
      - hive

  # Databases
  postgres:
    container_name: postgres
    image: postgres:13
    environment:
      POSTGRES_DB: retail_store
      POSTGRES_USER: retail_user
      POSTGRES_PASSWORD: retail_pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U retail_user -d retail_store"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - hive

  mongodb:
    image: mongo:6.0
    container_name: mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: retail_user
      MONGO_INITDB_ROOT_PASSWORD: retail_pass
      MONGO_INITDB_DATABASE: retail_realtime
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    ports:
      - "27017:27017"
    networks:
      - hive

  # Hive (PostgreSQL-backed)
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    environment:
      DB_TYPE: postgres
      DB_HOST: postgres
      DB_NAME: retail_store
      DB_USER: retail_user
      DB_PASS: retail_pass
      METASTORE_PORT: 9083
    depends_on:
      - postgres
    volumes:
      - ./hive-init.sh:/opt/hive-init.sh
    entrypoint: ["/opt/hive-init.sh"]
    ports:
      - "9083:9083"
    networks:
      - hive

  hive-server:
    image: apache/hive:4.0.0
    container_name: hive-server
    depends_on:
      - hive-metastore
    environment:
      SERVICE_NAME: hiveserver2
      METASTORE_URIS: thrift://hive-metastore:9083
      HIVE_WAREHOUSE_DIR: file:///opt/hive/warehouse
    volumes:
      - hive_data:/opt/hive/warehouse
    ports:
      - "10000:10000"  # JDBC
      - "10002:10002"  # Web UI
    networks:
      - hive

  # Processing
  spark:
    image: bitnami/spark:3.5
    container_name: spark-master
    ports:
      - "8085:8085"  # Spark UI
      - "7077:7077"  # Master port
    environment:
      SPARK_MODE: master
      SPARK_UI_PORT: 8085
    volumes:
      - ./spark-apps:/app
    networks:
      - hive

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on:
      - spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    volumes:
      - ./spark-apps:/app
    networks:
      - hive

  # Apache Airflow
  airflow-postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - hive

  airflow-init:
    image: apache/airflow:2.7.2
    container_name: airflow-init
    depends_on:
      - airflow-postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: |
      bash -c "
        airflow db init &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
        echo 'Airflow initialization completed'
      "
    networks:
      - hive

  airflow-webserver:
    image: apache/airflow:2.7.2
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    ports:
      - "8090:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - hive

  airflow-scheduler:
    image: apache/airflow:2.7.2
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - hive

  # Data Visualization
  superset:
    image: apache/superset
    container_name: superset
    restart: unless-stopped
    ports:
      - "8089:8089"
    environment:
      SUPERSET_SECRET_KEY: 'thisISaSECRET_key'
      ADMIN_USERNAME: admin
      ADMIN_PASSWORD: admin
      ADMIN_FIRST_NAME: Admin
      ADMIN_LAST_NAME: User
      ADMIN_EMAIL: admin@example.com
    volumes:
      - superset_data:/app/superset_data
    depends_on:
      - hive-server
    command: >
      /bin/sh -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin &&
        superset init &&
        superset run -h 0.0.0.0 -p 8089
      "
    networks:
      - hive
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Management UIs
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: localhost:9092
    networks:
      - hive

  mongo-express:
    image: mongo-express:latest
    container_name: mongo-express
    depends_on:
      - mongodb
    ports:
      - "8081:8081"
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: retail_user
      ME_CONFIG_MONGODB_ADMINPASSWORD: retail_pass
      ME_CONFIG_MONGODB_URL: mongodb://retail_user:retail_pass@mongodb:27017/
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin
    networks:
      - hive

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./pgadmin_servers.json:/pgadmin4/servers.json:ro
    restart: unless-stopped
    networks:
      - hive

  # Redis
  redis:
    image: redis:7.0
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --save 60 1 --loglevel warning
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - hive

  # RStudio (R Mage)
  rstudio:
    image: rocker/rstudio
    container_name: rstudio
    ports:
      - 8787:8787
    depends_on:
      - postgres
      - mongodb
    volumes:
      - rstudio-data:/home/rstudio
    working_dir: /home/rstudio
    entrypoint:
      - /usr/lib/rstudio-server/bin/rserver
    command:
      - --server-daemonize=0
      - --server-app-armor-enabled=0
    environment:
      - USER=rstudio
      - PASSWORD=rstudio
      - ROOT=true
    networks:
      - hive

volumes:
  postgres_data:
  mongodb_data:
  hive_data:
  superset_data:
  pgadmin_data:
  redis_data:
  rstudio-data:
  r_libs:
  airflow_postgres_data:

networks:
  hive:
    name: hive