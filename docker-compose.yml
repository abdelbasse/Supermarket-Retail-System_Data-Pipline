version: '3.8'

services:
  # Kafka Cluster
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    container_name: kafka
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_HOST://kafka:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_HOST
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
      - "29092:29092"

  # Databases
  postgres:
    container_name: postgres
    image: postgres:13
    environment:
      POSTGRES_DB: retail_store
      POSTGRES_USER: retail_user
      POSTGRES_PASSWORD: retail_pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    ports:
      - "5432:5432"
    healthcheck:  # <-- Add this block
      test: ["CMD-SHELL", "pg_isready -U retail_user -d retail_store"]
      interval: 5s
      timeout: 5s
      retries: 5

  mongodb:
    image: mongo:6.0
    container_name: mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: retail_user
      MONGO_INITDB_ROOT_PASSWORD: retail_pass
      MONGO_INITDB_DATABASE: retail_realtime
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    ports:
      - "27017:27017"

  # Hive (PostgreSQL-backed)
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    environment:
      DB_TYPE: postgres
      DB_HOST: postgres
      DB_NAME: retail_store
      DB_USER: retail_user
      DB_PASS: retail_pass
      METASTORE_PORT: 9083
    depends_on:
      - postgres
    volumes:
      - ./hive-init.sh:/opt/hive-init.sh
    entrypoint: ["/opt/hive-init.sh"]
    ports:
      - "9083:9083"

  hive-server:
    image: apache/hive:4.0.0
    container_name: hive-server
    depends_on:
      - hive-metastore
    environment:
      SERVICE_NAME: hiveserver2
      METASTORE_URIS: thrift://hive-metastore:9083
      HIVE_WAREHOUSE_DIR: file:///opt/hive/warehouse
    volumes:
      - hive_data:/opt/hive/warehouse
    ports:
      - "10000:10000"  # JDBC
      - "10002:10002"  # Web UI

  # Processing
  spark:
    image: bitnami/spark:3.5
    container_name: spark-master
    ports:
      - "8085:8085"  # Spark UI
      - "7077:7077"  # Master port
    environment:
      SPARK_MODE: master
      SPARK_UI_PORT: 8085
    volumes:
      - ./spark-apps:/app

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on:
      - spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    volumes:
      - ./spark-apps:/app

  # Data Visualization

  superset:
    image: apache/superset
    container_name: superset
    restart: unless-stopped
    ports:
      - "8089:8089"
    environment:
      SUPERSET_SECRET_KEY: 'thisISaSECRET_key'
      ADMIN_USERNAME: admin
      ADMIN_PASSWORD: admin
      ADMIN_FIRST_NAME: Admin
      ADMIN_LAST_NAME: User
      ADMIN_EMAIL: admin@example.com
    volumes:
      - superset_data:/app/superset_data
    depends_on:
      - hive-server
    command: >
      /bin/sh -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin &&
        superset init &&
        superset run -h 0.0.0.0 -p 8089
      "
    networks:
      - hive
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Management UIs
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: localhost:9092

  mongo-express:
    image: mongo-express:latest
    container_name: mongo-express
    depends_on:
      - mongodb
    ports:
      - "8081:8081"
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: retail_user
      ME_CONFIG_MONGODB_ADMINPASSWORD: retail_pass
      ME_CONFIG_MONGODB_URL: mongodb://retail_user:retail_pass@mongodb:27017/
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./pgadmin_servers.json:/pgadmin4/servers.json:ro
    restart: unless-stopped

  # Redis
  redis:
    image: redis:7.0
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --save 60 1 --loglevel warning
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Airflow
  airflow-scheduler:
    image: apache/airflow:2.6.3
    container_name: airflow-scheduler
    restart: unless-stopped
    depends_on:
      - redis
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://retail_user:retail_pass@postgres:5432/retail_store
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://retail_user:retail_pass@postgres:5432/retail_store
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    user: "${AIRFLOW_UID:-50000}:0"
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3

  airflow-webserver:
    image: apache/airflow:2.6.3
    container_name: airflow-webserver
    restart: unless-stopped
    depends_on:
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://retail_user:retail_pass@postgres:5432/retail_store
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://retail_user:retail_pass@postgres:5432/retail_store
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
    ports:
      - "8082:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    user: "${AIRFLOW_UID:-50000}:0"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-worker:
    image: apache/airflow:2.6.3
    container_name: airflow-worker
    restart: unless-stopped
    depends_on:
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://retail_user:retail_pass@postgres:5432/retail_store
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://retail_user:retail_pass@postgres:5432/retail_store
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    user: "${AIRFLOW_UID:-50000}:0"

  # RStudio (R Mage)
  rstudio:
    image: rocker/rstudio:4.3.1
    container_name: rstudio
    ports:
      - "8787:8787"
    environment:
      - PASSWORD=retail_pass
      - USER=retail_user
      - ROOT=TRUE
    volumes:
      - ./rstudio:/home/rstudio
      - ./r_libs:/usr/local/lib/R/site-library
    depends_on:
      - postgres
      - mongodb

volumes:
  postgres_data:
  mongodb_data:
  hive_data:
  superset_data:
  pgadmin_data:
  #added
  redis_data:
  airflow_logs:
  r_libs:


networks:
  hive:
    name: hive